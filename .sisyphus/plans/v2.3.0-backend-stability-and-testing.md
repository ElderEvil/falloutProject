# v2.3.0 Backend Stability & Testing - Implementation Plan

**Status:** Ready for Implementation  
**Created:** 2026-01-23  
**Branch:** `feat/v2.3.0`  
**Target Coverage:** 67% → 80%  
**Estimated Effort:** 2-3 days

---

## Executive Summary

v2.3.0 focuses on backend stability through test coverage improvements and code modernization. Current coverage: **44.5%** (measured), target: **80%**. Key deliverables:

1. **Test Coverage Expansion** - Add 535+ tests for untested modules (middleware, db, incident, game loop)
2. **Datetime Deprecation Fix** - Replace 68 `datetime.utcnow()` → `datetime.now(timezone.utc)`
3. **Unique Room Validation** - Comprehensive tests for existing uniqueness logic
4. **Test Infrastructure Improvements** - Fix session isolation, enhance fixtures

---

## Context from Research

### Current State
- **Total Tests:** 570 (535 backend passing)
- **Coverage:** 44.5% actual (ROADMAP claims 67% - needs verification)
- **Critical Gaps:**
  - `app/middleware/` - 0% (security, request_id)
  - `app/db/` - 0% (session, init_db)
  - `app/services/incident_service.py` - 28% (most tests skipped due to session issues)
  - `app/services/game_loop.py` - 54% (complex orchestration untested)

### Test Infrastructure (Already Excellent)
- ✅ pytest-asyncio with session-scoped event loop
- ✅ In-memory SQLite with transaction rollback
- ✅ Factory-boy pattern for test data
- ✅ AsyncClient with dependency overrides
- ✅ Mock external services (MinIO, Redis, OpenAI)

### Known Issues
- **Session isolation bug** in incident tests - fixtures not visible to queries inside service methods
- **datetime.utcnow() deprecation** - 68 instances across 21 files (Python 3.13+ warning)

---

## Phase 1: Datetime Deprecation Fix (Priority: HIGH)

**Scope:** Replace all `datetime.utcnow()` with `datetime.now(timezone.utc)`

### Files to Update (68 instances in 21 files)

#### Model Defaults (5 files)
```python
# BEFORE
from datetime import datetime
created_at: datetime = Field(default_factory=datetime.utcnow)

# AFTER  
from datetime import datetime, timezone
created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
```

**Files:**
- `app/models/base.py` - BaseModel, SoftDeleteModel timestamps
- `app/models/llm_interaction.py` - created_at default
- `app/models/chat_message.py` - timestamp default
- `app/models/incident.py` - start_time default
- `app/models/exploration.py` - get_utc_now() helper function

#### Service Logic (7 files)
```python
# BEFORE
now = datetime.utcnow()

# AFTER
now = datetime.now(timezone.utc)
```

**Files:**
- `app/services/training_service.py`
- `app/services/death_service.py`
- `app/services/breeding_service.py`
- `app/services/exploration/event_generator.py`
- `app/services/game_loop.py`
- `app/services/relationship_service.py`

#### Model Methods (5 files)
**Files:**
- `app/models/game_state.py`
- `app/models/pregnancy.py`
- `app/models/incident.py`
- `app/models/training.py`
- `app/models/exploration.py`

#### CRUD Operations (2 files)
**Files:**
- `app/crud/exploration.py`
- `app/crud/notification.py`

#### Test Files (5 files)
**Files:**
- `app/tests/test_services/test_death_service.py`
- `app/tests/test_services/test_incident_service.py`
- `app/tests/test_services/test_game_loop_exploration.py`
- `app/tests/test_services/test_exploration_service.py`
- `app/tests/test_services/test_breeding_service.py`

### Implementation Strategy
1. **Create utility helper** (optional - existing code uses inline):
   ```python
   # app/utils/datetime.py
   from datetime import datetime, timezone
   
   def utcnow() -> datetime:
       """Get current UTC datetime with timezone awareness."""
       return datetime.now(timezone.utc)
   ```
2. **Update imports** - Ensure `timezone` imported: `from datetime import datetime, timezone, timedelta`
3. **Replace all instances** - Use AST-grep or manual find/replace
4. **Run tests** - Verify no regressions

**Acceptance Criteria:**
- [ ] All 68 instances replaced
- [ ] All tests pass (`uv run pytest app/tests/`)
- [ ] No deprecation warnings when running app

---

## Phase 2: Middleware Testing (Priority: HIGH)

**Scope:** Add tests for `app/middleware/` (0% → 85%)

### 2.1 Security Middleware Tests

**File:** `app/tests/test_middleware/test_security.py`

**Test Cases (15 tests):**

```python
# Configuration Tests (5 tests)
- test_create_security_config_default_settings
- test_create_security_config_with_ipinfo_token
- test_create_security_config_ipinfo_initialization_error
- test_create_security_config_redis_enabled_in_production
- test_create_security_config_redis_disabled_in_local

# Rate Limiting Tests (5 tests)
- test_rate_limit_allows_requests_under_limit
- test_rate_limit_blocks_requests_over_limit
- test_rate_limit_resets_after_window
- test_rate_limit_uses_redis_in_production
- test_auto_ban_after_threshold_exceeded

# IP Filtering Tests (5 tests)
- test_whitelist_ip_bypasses_rate_limit
- test_blacklist_ip_blocked
- test_geolocation_handler_blocks_country (if IPInfo enabled)
- test_blocked_user_agents_rejected
- test_cloud_provider_blocking (when enabled)
```

**Patterns:**
```python
import pytest
from fastapi import FastAPI
from httpx import AsyncClient, ASGITransport
from unittest.mock import MagicMock, patch

from app.middleware.security import create_security_config

@pytest.mark.asyncio
async def test_create_security_config_default_settings():
    """Test security config creation with default settings"""
    config = create_security_config()
    
    assert config.rate_limit == settings.RATE_LIMIT_REQUESTS
    assert config.rate_limit_window == settings.RATE_LIMIT_WINDOW
    assert config.auto_ban_threshold == settings.AUTO_BAN_THRESHOLD
    assert config.enable_redis is False  # local environment

@pytest.mark.asyncio  
async def test_rate_limit_blocks_requests_over_limit(async_client: AsyncClient):
    """Test rate limiting blocks excessive requests"""
    # Make requests up to limit
    for _ in range(settings.RATE_LIMIT_REQUESTS):
        response = await async_client.get("/api/v1/system/info")
        assert response.status_code == 200
    
    # Next request should be rate limited
    response = await async_client.get("/api/v1/system/info")
    assert response.status_code == 429  # Too Many Requests
```

### 2.2 Request ID Middleware Tests

**File:** `app/tests/test_middleware/test_request_id.py`

**Test Cases (8 tests):**

```python
# Header Generation Tests (4 tests)
- test_generates_request_id_when_missing
- test_preserves_existing_request_id
- test_request_id_added_to_response_headers
- test_request_id_format_is_valid_uuid

# State Management Tests (4 tests)
- test_request_id_stored_in_state
- test_request_id_accessible_in_endpoint
- test_handles_non_http_scope_gracefully
- test_concurrent_requests_have_unique_ids
```

**Target:** 23 tests total for middleware

---

## Phase 3: Database Layer Testing (Priority: HIGH)

**Scope:** Add tests for `app/db/` (0% → 75%)

### 3.1 Session Management Tests

**File:** `app/tests/test_db/test_session.py`

**Test Cases (6 tests):**

```python
# Session Lifecycle Tests (3 tests)
- test_get_async_session_creates_session
- test_async_session_context_manager_closes_properly
- test_async_session_handles_connection_errors

# Connection Pooling Tests (3 tests)
- test_multiple_sessions_from_same_engine
- test_session_isolation_between_requests
- test_session_cleanup_on_exception
```

**Pattern:**
```python
@pytest.mark.asyncio
async def test_get_async_session_creates_session():
    """Test async session generator creates valid session"""
    async for session in get_async_session():
        assert isinstance(session, AsyncSession)
        assert session.is_active
        break
```

### 3.2 Database Initialization Tests

**File:** `app/tests/test_db/test_init_db.py`

**Test Cases (10 tests):**

```python
# User Creation Tests (3 tests)
- test_init_db_creates_superuser
- test_init_db_creates_test_user
- test_init_db_skips_existing_users

# Vault Seeding Tests (3 tests)
- test_init_db_creates_vault_for_test_user
- test_init_db_creates_rooms_for_vault
- test_init_db_creates_dwellers_for_rooms

# Equipment Seeding Tests (2 tests)
- test_init_db_creates_outfits_for_dwellers
- test_init_db_creates_weapons_for_dwellers

# Error Handling Tests (2 tests)
- test_init_db_handles_database_errors
- test_init_db_logs_creation_events
```

**Pattern:**
```python
@pytest.mark.asyncio
async def test_init_db_creates_superuser(async_session: AsyncSession):
    """Test init_db creates superuser correctly"""
    await init_db(async_session)
    
    user = await crud.user.get_by_email(
        email=settings.FIRST_SUPERUSER_EMAIL,
        db_session=async_session
    )
    
    assert user is not None
    assert user.is_superuser is True
    assert user.username == settings.FIRST_SUPERUSER_USERNAME
```

**Target:** 16 tests total for db layer

---

## Phase 4: Incident Service Testing (Priority: MEDIUM)

**Scope:** Fix session isolation + add tests (28% → 75%)

### 4.1 Fix Session Isolation Bug

**Problem:** Skipped tests show fixtures not visible to service queries

**Root Cause:** Service methods create new queries that don't see fixture data committed in test transaction

**Solution:** Explicitly commit fixture data before calling service methods

```python
# BEFORE (test skipped)
@pytest.mark.skip(reason="Session isolation issue")
@pytest.mark.asyncio
async def test_spawn_incident_success(async_session, vault, room):
    incident = await incident_service.spawn_incident(async_session, vault.id)
    assert incident is not None

# AFTER (fixed)
@pytest.mark.asyncio
async def test_spawn_incident_success(async_session, vault, room):
    # Explicitly commit fixture data so service can see it
    await async_session.commit()
    
    incident = await incident_service.spawn_incident(async_session, vault.id)
    assert incident is not None
    assert incident.vault_id == vault.id
```

### 4.2 Incident Service Tests

**File:** `app/tests/test_services/test_incident_service.py` (expand existing)

**New Test Cases (25 tests):**

```python
# Spawn Logic Tests (8 tests)
- test_should_spawn_incident_requires_minimum_population
- test_should_spawn_incident_respects_cooldown
- test_should_spawn_incident_random_chance
- test_spawn_incident_selects_valid_room
- test_spawn_incident_assigns_correct_difficulty
- test_spawn_incident_spawns_at_vault_door_for_raiders
- test_spawn_incident_creates_incident_record
- test_spawn_incident_updates_vault_last_incident_time

# Combat Calculation Tests (7 tests)
- test_calculate_dweller_combat_power_with_stats
- test_calculate_dweller_combat_power_with_weapon_bonus
- test_calculate_raider_power_scales_with_difficulty
- test_process_incident_applies_damage_to_dwellers
- test_process_incident_applies_damage_to_raiders
- test_process_incident_triggers_death_on_zero_health
- test_process_incident_awards_xp_on_victory

# Spread Mechanics Tests (4 tests)
- test_spread_incident_finds_adjacent_rooms
- test_spread_incident_respects_spread_chance
- test_spread_incident_only_spreads_to_unaffected_rooms
- test_spread_incident_creates_new_incident_records

# Loot & Rewards Tests (3 tests)
- test_generate_loot_scales_with_difficulty
- test_award_combat_xp_distributes_to_participants
- test_resolve_incident_manually_costs_caps

# Edge Cases (3 tests)
- test_spawn_incident_returns_none_when_no_rooms
- test_process_incident_handles_empty_room
- test_process_incident_ends_when_all_raiders_dead
```

**Target:** 25+ new tests (current skipped tests unskipped + new coverage)

---

## Phase 5: Game Loop Testing (Priority: MEDIUM)

**Scope:** Add comprehensive tests (54% → 80%)

### 5.1 Game Loop Service Tests

**File:** `app/tests/test_services/test_game_loop.py` (create new)

**Test Cases (30 tests):**

```python
# Orchestration Tests (5 tests)
- test_process_game_tick_processes_all_active_vaults
- test_process_vault_tick_runs_all_phases
- test_process_vault_tick_handles_errors_gracefully
- test_process_vault_tick_skips_paused_vaults
- test_process_vault_tick_aggregates_results

# Exploration Phase Tests (5 tests)
- test_process_explorations_auto_completes_expired
- test_process_explorations_awards_loot_and_xp
- test_process_explorations_generates_events
- test_process_explorations_updates_vault_resources
- test_process_explorations_handles_dweller_death

# Dweller Health Phase Tests (5 tests)
- test_process_dwellers_checks_health_death
- test_process_dwellers_checks_radiation_death
- test_process_dwellers_awards_work_xp
- test_process_dwellers_skips_dead_dwellers
- test_process_dwellers_updates_dweller_stats

# Training Phase Tests (4 tests)
- test_process_training_updates_progress
- test_process_training_completes_finished_sessions
- test_process_training_increases_special_stats
- test_process_training_frees_trainer_on_completion

# Incident Phase Tests (4 tests)
- test_process_incidents_spawns_new_incidents
- test_process_incidents_processes_active_incidents
- test_process_incidents_spreads_incidents
- test_process_incidents_respects_spawn_cooldown

# Relationship Phase Tests (3 tests)
- test_update_room_relationships_increases_affinity
- test_update_room_relationships_only_for_same_room
- test_update_room_relationships_respects_max_affinity

# Pregnancy & Birth Phase Tests (4 tests)
- test_process_pregnancies_and_births_checks_conception
- test_process_pregnancies_and_births_delivers_babies
- test_process_pregnancies_and_births_moves_mother_to_living_quarters
- test_age_children_converts_to_adults
```

**Pattern:**
```python
@pytest.mark.asyncio
async def test_process_explorations_auto_completes_expired(
    async_session, vault, dweller
):
    """Test that explorations auto-complete after duration expires"""
    # Create exploration that started 2 hours ago (expired)
    exploration = await crud.exploration.create_with_dweller_stats(
        async_session,
        vault_id=vault.id,
        dweller_id=dweller.id,
        duration=1  # 1 hour duration
    )
    exploration.start_time = datetime.now(timezone.utc) - timedelta(hours=2)
    await async_session.commit()
    
    initial_caps = vault.bottle_caps
    
    # Process explorations
    result = await game_loop_service._process_explorations(
        async_session, vault.id
    )
    
    await async_session.refresh(vault)
    await async_session.refresh(exploration)
    
    assert result["completed"] == 1
    assert exploration.status == ExplorationStatus.COMPLETED
    assert vault.bottle_caps > initial_caps  # Loot awarded
```

**Target:** 30+ tests for game loop

---

## Phase 6: Unique Room Verification (Priority: LOW)

**Scope:** Comprehensive tests for existing uniqueness logic

### 6.1 Unique Room Tests

**File:** `app/tests/test_api/test_room.py` (expand existing)

**New Test Cases (8 tests):**

```python
# Uniqueness Logic Tests (4 tests)
- test_is_unique_property_true_when_no_incremental_cost
- test_is_unique_property_false_when_has_incremental_cost
- test_build_unique_room_fails_when_already_exists
- test_build_non_unique_room_allows_multiple

# Buildable Rooms Filtering Tests (4 tests)
- test_buildable_rooms_excludes_built_unique_rooms
- test_buildable_rooms_includes_unbuilt_unique_rooms
- test_buildable_rooms_includes_non_unique_rooms_always
- test_buildable_rooms_filters_by_level_requirements
```

**Pattern:**
```python
@pytest.mark.asyncio
async def test_build_unique_room_fails_when_already_exists(
    async_client: AsyncClient,
    superuser_token_headers: dict,
    vault: Vault
):
    """Test that building a second unique room raises error"""
    # Build first unique room (e.g., Vault Door)
    room_data = {
        "name": "Vault Door",
        "vault_id": str(vault.id),
        "level": 1
    }
    
    response1 = await async_client.post(
        "/api/v1/rooms/build/",
        json=room_data,
        headers=superuser_token_headers
    )
    assert response1.status_code == 201
    
    # Attempt to build second Vault Door
    response2 = await async_client.post(
        "/api/v1/rooms/build/",
        json=room_data,
        headers=superuser_token_headers
    )
    assert response2.status_code == 400
    assert "unique room" in response2.json()["detail"].lower()
```

**Target:** 8 tests for unique room verification

---

## Phase 7: Test Infrastructure Improvements (Priority: LOW)

**Scope:** Enhance existing test infrastructure based on 2026 best practices

### 7.1 Update pytest Configuration

**File:** `backend/pyproject.toml`

**Changes:**
```toml
[tool.pytest.ini_options]
testpaths = ["app/tests"]
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
]
addopts = [
    "--strict-markers",
    "--strict-config",
    "--cov=app",
    "--cov-report=term-missing:skip-covered",
    "--cov-report=html:htmlcov",
    "--cov-report=json",
    "--cov-fail-under=80",
    "-v",
]

[tool.coverage.run]
source = ["app"]
omit = [
    "*/tests/*",
    "*/migrations/*",
    "*/__pycache__/*",
    "*/conftest.py",
]
branch = true
parallel = true

[tool.coverage.report]
precision = 2
show_missing = true
skip_covered = false
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
]
```

### 7.2 Optional: Migrate to pytest.mark.anyio

**Rationale:** FastAPI 2026 docs recommend `anyio` for async tests

**Impact:** All `@pytest.mark.asyncio` → `@pytest.mark.anyio`

**Decision:** SKIP for v2.3.0 - keep existing pattern (works well, low ROI for migration)

---

## Test Count Summary

| Module | Current Tests | New Tests | Total | Target Coverage |
|--------|--------------|-----------|-------|-----------------|
| middleware/ | 0 | 23 | 23 | 85% |
| db/ | 0 | 16 | 16 | 75% |
| incident_service | ~10 (skipped) | 25 | 35 | 75% |
| game_loop | ~10 (indirect) | 30 | 40 | 80% |
| room (unique) | ~15 | 8 | 23 | 95% |
| **TOTAL** | **570** | **102** | **672** | **80%** |

**Coverage Trajectory:** 44.5% → 80% (target)

---

## Implementation Checklist

### Phase 1: Datetime Fix ✅
- [ ] Update model defaults (5 files)
- [ ] Update service logic (7 files)
- [ ] Update model methods (5 files)
- [ ] Update CRUD operations (2 files)
- [ ] Update test files (5 files)
- [ ] Verify imports include `timezone`
- [ ] Run full test suite
- [ ] Verify no deprecation warnings

### Phase 2: Middleware Testing ✅
- [ ] Create `test_middleware/` directory
- [ ] Write `test_security.py` (15 tests)
- [ ] Write `test_request_id.py` (8 tests)
- [ ] Run: `pytest app/tests/test_middleware/ -v`
- [ ] Coverage target: 85%

### Phase 3: Database Testing ✅
- [ ] Create `test_db/` directory
- [ ] Write `test_session.py` (6 tests)
- [ ] Write `test_init_db.py` (10 tests)
- [ ] Run: `pytest app/tests/test_db/ -v`
- [ ] Coverage target: 75%

### Phase 4: Incident Service Testing ✅
- [ ] Fix session isolation in existing tests
- [ ] Unskip all skipped tests
- [ ] Add spawn logic tests (8 tests)
- [ ] Add combat calculation tests (7 tests)
- [ ] Add spread mechanics tests (4 tests)
- [ ] Add loot/rewards tests (3 tests)
- [ ] Add edge case tests (3 tests)
- [ ] Run: `pytest app/tests/test_services/test_incident_service.py -v`
- [ ] Coverage target: 75%

### Phase 5: Game Loop Testing ✅
- [ ] Create `test_game_loop.py`
- [ ] Write orchestration tests (5 tests)
- [ ] Write exploration phase tests (5 tests)
- [ ] Write dweller health phase tests (5 tests)
- [ ] Write training phase tests (4 tests)
- [ ] Write incident phase tests (4 tests)
- [ ] Write relationship phase tests (3 tests)
- [ ] Write pregnancy/birth phase tests (4 tests)
- [ ] Run: `pytest app/tests/test_services/test_game_loop.py -v`
- [ ] Coverage target: 80%

### Phase 6: Unique Room Testing ✅
- [ ] Add uniqueness logic tests (4 tests)
- [ ] Add buildable filtering tests (4 tests)
- [ ] Run: `pytest app/tests/test_api/test_room.py -v`
- [ ] Coverage target: 95%

### Phase 7: Infrastructure Improvements ✅
- [ ] Update `pyproject.toml` pytest config
- [ ] Update `pyproject.toml` coverage config
- [ ] Add test markers documentation
- [ ] Run full suite with new config

### Final Verification ✅
- [ ] Run full test suite: `uv run pytest app/tests/ -v`
- [ ] Check coverage: `uv run pytest app/tests/ --cov=app --cov-report=term-missing`
- [ ] Verify coverage ≥ 80%
- [ ] Run linting: `uv run ruff check . && uv run ruff format .`
- [ ] Update ROADMAP.md with completion
- [ ] Commit changes
- [ ] Create PR to master

---

## Success Criteria

1. **Coverage Target Met:** Backend test coverage ≥ 80%
2. **All Tests Pass:** 672+ tests passing
3. **No Deprecation Warnings:** All `datetime.utcnow()` replaced
4. **Session Isolation Fixed:** Incident tests no longer skipped
5. **Unique Rooms Verified:** Comprehensive test coverage confirms existing logic works
6. **CI/CD Green:** All workflows pass

---

## Risk Mitigation

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Session isolation fix breaks tests | Medium | High | Incremental fix + rollback plan |
| Datetime changes cause timestamp bugs | Low | High | Comprehensive test coverage before/after |
| Coverage target too ambitious | Low | Medium | Prioritize critical modules first |
| Incident/game loop tests too complex | Medium | Medium | Start with simple cases, iterate |

---

## Dependencies

- No external library updates required
- Existing test infrastructure sufficient
- pytest-asyncio, pytest-cov already installed
- Factory-boy patterns already established

---

## Out of Scope (Future Work)

- Frontend test coverage improvements (separate sprint)
- Performance testing with Locust (v2.4.0+)
- MinIO → RustFS migration (technical debt backlog)
- Component refactoring (DwellerCard, RoomGrid - P3)
- Motion Vue integration (v2.4.0)

---

## Questions for User

1. **Coverage verification:** ROADMAP says 67% but I measured 44.5%. Should I re-measure or trust ROADMAP?
2. **Test priority:** Should I focus on middleware+db (easier wins) or incident+game_loop (higher complexity) first?
3. **Session isolation fix:** Is it acceptable to add explicit `await async_session.commit()` in tests, or prefer a fixture-level solution?
4. **anyio migration:** Worth migrating from `@pytest.mark.asyncio` to `@pytest.mark.anyio` now or defer?

---

**END OF PLAN**
